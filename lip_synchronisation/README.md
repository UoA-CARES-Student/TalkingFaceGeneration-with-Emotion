# Lip synchronisation stage of TalkingFaceGeneration-with-Emotion 
Lip synchronisation generator takes an image from the emotion generator and an audio file generated by Text-to-Speech method. 

## Installation
Use google colab notebook provided (name) 
OR
use Conda environment (python 3.8)
```bash
1. conda create -n lipsync python=3.8
2. conda activate lipsync
```
Clone directory and 
```bash
1. pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
2. conda install ffmpeg
3. pip install -r requirements.txt
4. python -m pip install paddlepaddle-gpu==2.3.2.post112 \ -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
```

## Download and extract checkpoints
Download checkpoints from 
google drive: https://drive.google.com/file/d/1lW4mf5YNtS4MAD7ZkAauDDWp2N3_Qzs7/view
And extract to project path 
 ```bash
tar -zxvf checkpoints.tar.gz
```
Google colab demo already includes downloading and extracting checkpoints 

## Inference 
 ```bash
--driven_audio <audio.wav or .mp3> \
                    --source_video <video.mp4 or .avi> \
                    --enhancer <none,lip,face> \  # default is lip 
                    --use_DAIN \  # for temporal inference, but will take much longer time 
             		--time_step 0.5 #Frame insertion frequency, default 0.5
```

## Acknowledgments 